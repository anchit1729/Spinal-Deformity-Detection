{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UNet++ Transformations\n",
    "\n",
    "This notebook implements some basic transformations required for the dataset to be passed to UNet successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Callable, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import albumentations as A\n",
    "from sklearn.externals._pilutil import bytescale\n",
    "from skimage.util import crop\n",
    "\n",
    "def normalize_01(inp: np.ndarray):\n",
    "    # squash image input to the value range [0, 1] (no clipping)\n",
    "    inp_out = (inp - np.min(inp)) / np.ptp(inp)\n",
    "    return inp_out\n",
    "\n",
    "def normalize(inp: np.ndarray, mean: float, std: float):\n",
    "    # normalize based on mean and standard deviation\n",
    "    inp_out = (inp - mean) / std\n",
    "    return inp_out\n",
    "\n",
    "def create_dense_target(tar: np.ndarray):\n",
    "    classes = np.unique(tar)\n",
    "    dummy = np.zeros_like(tar)\n",
    "    for idx, value in enumerate(classes):\n",
    "        mask = np.where(tar == value)\n",
    "        dummy[mask] = idx\n",
    "        \n",
    "    return dummy\n",
    "\n",
    "def center_crop_to_size(x: np.ndarray, size: Tuple, copy: bool = False) -> np.ndarray:\n",
    "    # center crop a given array x to the size passed in the function\n",
    "    # expects even spatial dimensions!\n",
    "    x_shape = np.array(x.shape)\n",
    "    size = np.array(size)\n",
    "    params_list = ((x_shape - size) / 2).astype(np.int).tolist()\n",
    "    params_tuple = tuple([(i, i) for i in params_list])\n",
    "    cropped_image = crop(x, crop_width=params_tuple, copy=copy)\n",
    "    return cropped_image\n",
    "\n",
    "def re_normalize(inp: np.ndarray, low: int = 0, high: int = 255):\n",
    "    # normalize the data to a certain range\n",
    "    # default: [0-255]\n",
    "    inp_out = bytescale(inp, low=low, high=high)\n",
    "    return inp_out\n",
    "\n",
    "def random_flip(inp: np.ndarray, tar: np.ndarray, ndim_spatial: int):\n",
    "    flip_dims = [np.random.randint(low=0, high=2) for dim in range(ndim_spatial)]\n",
    "    \n",
    "    flip_dims_inp = tuple([i + 1 for i, element in enumerate(flip_dims) if element == 1])\n",
    "    flip_dims_tar = tuple([i for i, element in enumerate(flip_dims) if element == 1])\n",
    "    \n",
    "    inp_flipped = np.flip(inp, axis=flip_dims_inp)\n",
    "    tar_flipped = np.flip(tar, axis=flip_dims_tar)\n",
    "    \n",
    "    return inp_flipped, tar_flipped\n",
    "\n",
    "\n",
    "class Repr:\n",
    "    # evaluable string representation of an object\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f'{self.__class__.__name__}: {self.__dict__}'\n",
    "    \n",
    "    \n",
    "class FunctionWrapperSingle(Repr):\n",
    "    # a function wrapper that returns a partial for input only\n",
    "    \n",
    "    def __init__(self, function: Callable, *args, **kwargs):\n",
    "        from functools import partial\n",
    "        self.function = partial(function, *args, **kwargs)\n",
    "        \n",
    "    def __call__(self, inp: np.ndarray):\n",
    "        return self.function(inp)\n",
    "\n",
    "    \n",
    "class FunctionWrapperDouble(Repr):\n",
    "    # a function wrapper that returns a partial for an input-target pair\n",
    "    \n",
    "    def __init__(self, function: Callable, input: bool = True, target: bool = False, *args, **kwargs):\n",
    "        from functools import partial\n",
    "        self.function = partial(function, *args, **kwargs)\n",
    "        self.input = input\n",
    "        self.target = target\n",
    "        \n",
    "    def __call__(self, inp: np.ndarray, tar: dict):\n",
    "        if self.input: \n",
    "            inp = self.function(inp)\n",
    "        if self.target:\n",
    "            tar = self.function(tar)\n",
    "        return inp, tar\n",
    "    \n",
    "\n",
    "class Compose:\n",
    "    # baseclass - composes several transforms together\n",
    "    \n",
    "    def __init__(self, transforms: List[Callable]):\n",
    "        self.transforms = transforms\n",
    "        \n",
    "    def __repr__(self): \n",
    "        return str([transform for transform in transforms])\n",
    "    \n",
    "\n",
    "class ComposeDouble(Compose):\n",
    "    # composes transforms for input-target pairs\n",
    "    \n",
    "    def __call__(self, inp: np.ndarray, target: dict):\n",
    "        for t in self.transforms:\n",
    "            inp, target = t(inp, target)\n",
    "        return inp, target\n",
    "    \n",
    "\n",
    "class ComposeSingle(Compose):\n",
    "    # composes transforms for input only\n",
    "    \n",
    "    def __call__(self, inp: np.ndarray):\n",
    "        for t in self.transforms:\n",
    "            inp = t(inp)\n",
    "        return inp\n",
    "    \n",
    "class AlbuSeg2d(Repr):\n",
    "    # wrapper for albumentations' segmentation-compatible 2D augmentations\n",
    "    # wraps an augmentation so it can be used within the provided transform pipeline\n",
    "    # see https://github.com/albu/albumentations for more information\n",
    "    # expected input: (C, spatial_dims)\n",
    "    # expected target: (spatial_dims) -> No (C)hannel dimension\n",
    "    \n",
    "    def __init__(self, albumentation: Callable):\n",
    "        self.albumentation = albumentation\n",
    "        \n",
    "    def __call__(self, inp: np.ndarray, tar: np.ndarray):\n",
    "        # input, target\n",
    "        out_dict = self.albumentation(image=inp, mask=tar)\n",
    "        input_out = out_dict['image']\n",
    "        target_out = out_dict['mask']\n",
    "        \n",
    "        return input_out, target_out\n",
    "    \n",
    "class AlbuSeg3d(Repr):\n",
    "    # wrapper for albumentations' segmentation-compatible 2D augmentations.\n",
    "    # wraps an augmentation so it can be used within the provided transform pipeline.\n",
    "    # see https://github.com/albu/albumentations for more information.\n",
    "    # expected input: (spatial_dims)  -> No (C)hannel dimension\n",
    "    # expected target: (spatial_dims) -> No (C)hannel dimension\n",
    "    # iterates over the slices of a input-target pair stack and performs the same albumentation function.\n",
    "    \n",
    "    def __init__(self, albumentation: Callable):\n",
    "        self.albumentation = A.ReplayCompose([albumentation])\n",
    "        \n",
    "    def __call__(self, inp: np.ndarray, tar: np.ndarray):\n",
    "        # input, target\n",
    "        # target has to be in uint8\n",
    "        tar = tar.astype(np.uint8)\n",
    "        \n",
    "        input_copy = np.copy(inp)\n",
    "        target_copy = np.copy(tar)\n",
    "        \n",
    "        # perform an albu on one slice and access the replay dict\n",
    "        replay_dict = self.albumentation(image=inp[0])['replay']\n",
    "        \n",
    "        # todo: consider cases with rgb 3d or multimodal 3d input\n",
    "        # only if input_shape == target_shape\n",
    "        for index, (input_slice, target_slice) in enumerate(zip(inp, tar)):\n",
    "            result = A.ReplayCompose.replay(replay_dict, image=input_slice, mask=target_slice)\n",
    "            input_copy[index] = result['image']\n",
    "            target_copy[index] = result['mask']\n",
    "            \n",
    "        return input_copy, target_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code segment defines classes and functions that enable composition of transforms. These are needed to perform preprocessing on the input images before passing to the model. The following code segment tests out the functionality of the defined classes+functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from skimage.transform import resize\n",
    "\n",
    "x = np.random.randint(0, 256, size=(128, 128, 3), dtype=np.uint8)\n",
    "y = np.random.randint(10, 15, size=(128, 128), dtype=np.uint8)\n",
    "\n",
    "transforms = ComposeDouble([\n",
    "    FunctionWrapperDouble(resize, input=True, target=False, output_shape=(64, 64, 3)),\n",
    "    FunctionWrapperDouble(resize, input=False, target=True, output_shape=(64, 64), order=0, anti_aliasing=False, preserve_range=True),\n",
    "    FunctionWrapperDouble(create_dense_target, input=False, target=True),\n",
    "    FunctionWrapperDouble(np.moveaxis, input=True, target=False, source=-1, destination=0),\n",
    "    FunctionWrapperDouble(normalize_01)\n",
    "])\n",
    "\n",
    "x_t, y_t = transforms(x, y)\n",
    "\n",
    "print(f'x = shape: {x.shape}; type: {x.dtype}')\n",
    "print(f'x = min: {x.min()}; max: {x.max()}')\n",
    "print(f'x_t = shape: {x_t.shape}; type: {x_t.dtype}')\n",
    "print(f'x_t = min: {x_t.min()}; max: {x_t.max()}')\n",
    "\n",
    "print(f'y = shape: {y.shape}; class: {np.unique(y)}')\n",
    "print(f'y_t = shape: {y_t.shape}; class: {np.unique(y_t)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
